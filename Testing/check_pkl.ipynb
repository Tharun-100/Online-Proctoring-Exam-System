{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODULE:1 FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLS OR LIBRARIES REQUIRED\n",
    "# MEDIAPIPE FACE DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# =========================\n",
    "# MODEL INITIALIZATION\n",
    "# =========================\n",
    "\n",
    "base_face_det = python.BaseOptions(model_asset_path=r\"E:\\Projects in ML\\FRAUD DETECTION SYSTEM FOR THE ONLINE PROCTORED EXAMS\\mediapipe_models\\blaze_face_short_range.tflite\")\n",
    "base_face_lm  = python.BaseOptions(model_asset_path=r\"E:\\Projects in ML\\FRAUD DETECTION SYSTEM FOR THE ONLINE PROCTORED EXAMS\\mediapipe_models\\face_landmarker.task\")\n",
    "base_hand     = python.BaseOptions(model_asset_path=r\"E:\\Projects in ML\\FRAUD DETECTION SYSTEM FOR THE ONLINE PROCTORED EXAMS\\mediapipe_models\\hand_landmarker.task\")\n",
    "\n",
    "face_detector = vision.FaceDetector.create_from_options(\n",
    "    vision.FaceDetectorOptions(\n",
    "        base_options=base_face_det,\n",
    "        running_mode=vision.RunningMode.IMAGE\n",
    "    )\n",
    ")\n",
    "\n",
    "face_landmarker = vision.FaceLandmarker.create_from_options(\n",
    "    vision.FaceLandmarkerOptions(\n",
    "        base_options=base_face_lm,\n",
    "        output_facial_transformation_matrixes=True,\n",
    "        running_mode=vision.RunningMode.IMAGE\n",
    "    )\n",
    ")\n",
    "\n",
    "hand_landmarker = vision.HandLandmarker.create_from_options(\n",
    "    vision.HandLandmarkerOptions(\n",
    "        base_options=base_hand,\n",
    "        num_hands=2,\n",
    "        running_mode=vision.RunningMode.IMAGE\n",
    "    )\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# UTILITY FUNCTIONS\n",
    "# =========================\n",
    "\n",
    "def euclidean(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "def compute_head_pose(lm, w, h):\n",
    "    try:\n",
    "        required_ids = [1, 33, 61, 152, 263, 291]\n",
    "        for idx in required_ids:\n",
    "            if idx >= len(lm):\n",
    "                return None, None, None\n",
    "\n",
    "        model_points = np.array([\n",
    "            (0, 0, 0),\n",
    "            (0, -330, -65),\n",
    "            (-225, 170, -135),\n",
    "            (225, 170, -135),\n",
    "            (-150, -150, -125),\n",
    "            (150, -150, -125)\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        image_points = np.array([\n",
    "            (lm[1].x * w, lm[1].y * h),\n",
    "            (lm[152].x * w, lm[152].y * h),\n",
    "            (lm[33].x * w, lm[33].y * h),\n",
    "            (lm[263].x * w, lm[263].y * h),\n",
    "            (lm[61].x * w, lm[61].y * h),\n",
    "            (lm[291].x * w, lm[291].y * h)\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        cam = np.array([\n",
    "            [w, 0, w / 2],\n",
    "            [0, w, h / 2],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        dist = np.zeros((4, 1))\n",
    "\n",
    "        success, rvec, _ = cv2.solvePnP(\n",
    "            model_points,\n",
    "            image_points,\n",
    "            cam,\n",
    "            dist,\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE\n",
    "        )\n",
    "\n",
    "        if not success:\n",
    "            return None, None, None\n",
    "\n",
    "        rmat, _ = cv2.Rodrigues(rvec)\n",
    "        pitch, yaw, roll = cv2.RQDecomp3x3(rmat)[0]\n",
    "        return pitch, yaw, roll\n",
    "\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "def head_pose_label(pitch, yaw):\n",
    "    if abs(pitch) < 10 and abs(yaw) < 10:\n",
    "        return \"forward\"\n",
    "    if yaw > 10:\n",
    "        return \"right\"\n",
    "    if yaw < -10:\n",
    "        return \"left\"\n",
    "    if pitch > 10:\n",
    "        return \"down\"\n",
    "    return \"None\"\n",
    "\n",
    "# =========================\n",
    "# MAIN FEATURE EXTRACTION\n",
    "# =========================\n",
    "\n",
    "def extract_features(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w, _ = img.shape\n",
    "    mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # -------- FACE DETECTION --------\n",
    "    face_res = face_detector.detect(mp_img)\n",
    "    features[\"no_of_face\"] = len(face_res.detections)\n",
    "    features[\"face_present\"] = 1 if face_res.detections else 0\n",
    "\n",
    "    if not face_res.detections:\n",
    "        return features\n",
    "\n",
    "    det = face_res.detections[0]\n",
    "    bbox = det.bounding_box\n",
    "\n",
    "    features.update({\n",
    "        \"face_x\": bbox.origin_x,\n",
    "        \"face_y\": bbox.origin_y,\n",
    "        \"face_w\": bbox.width,\n",
    "        \"face_h\": bbox.height,\n",
    "        \"face_conf\": det.categories[0].score * 100\n",
    "    })\n",
    "\n",
    "    # -------- FACE LANDMARKS --------\n",
    "    mesh = face_landmarker.detect(mp_img)\n",
    "    lm = mesh.face_landmarks[0]\n",
    "\n",
    "    features.update({\n",
    "        \"left_eye_x\": lm[33].x * w,\n",
    "        \"left_eye_y\": lm[33].y * h,\n",
    "        \"right_eye_x\": lm[263].x * w,\n",
    "        \"right_eye_y\": lm[263].y * h,\n",
    "        \"nose_tip_x\": lm[1].x * w,\n",
    "        \"nose_tip_y\": lm[1].y * h,\n",
    "        \"mouth_x\": lm[13].x * w,\n",
    "        \"mouth_y\": lm[13].y * h\n",
    "    })\n",
    "\n",
    "    # -------- PUPILS & GAZE --------\n",
    "    plx, ply = lm[468].x * w, lm[468].y * h\n",
    "    prx, pry = lm[473].x * w, lm[473].y * h\n",
    "\n",
    "    gaze_x = (plx + prx) / 2\n",
    "    gaze_y = (ply + pry) / 2\n",
    "\n",
    "    if gaze_x < 0.4 * w:\n",
    "        gaze_dir = \"bottom_left\"\n",
    "    elif gaze_x > 0.6 * w:\n",
    "        gaze_dir = \"bottom_right\"\n",
    "    else:\n",
    "        gaze_dir = \"center\"\n",
    "\n",
    "    features.update({\n",
    "        \"pupil_left_x\": plx,\n",
    "        \"pupil_left_y\": ply,\n",
    "        \"pupil_right_x\": prx,\n",
    "        \"pupil_right_y\": pry,\n",
    "        \"gazePoint_x\": gaze_x,\n",
    "        \"gazePoint_y\": gaze_y,\n",
    "        \"gaze_direction\": gaze_dir,\n",
    "        \"gaze_on_script\": 1 if gaze_dir == \"center\" else 0\n",
    "    })\n",
    "\n",
    "    # -------- HEAD POSE --------\n",
    "    # pitch, yaw, roll = compute_head_pose(lm, w, h)\n",
    "    # features.update({\n",
    "    #     \"head_pitch\": pitch,\n",
    "    #     \"head_yaw\": yaw,\n",
    "    #     \"head_roll\": roll,\n",
    "    #     \"head_pose\": head_pose_label(pitch, yaw)\n",
    "    # })\n",
    "    pitch, yaw, roll = compute_head_pose(lm, w, h)\n",
    "\n",
    "    if pitch is None:\n",
    "        features.update({\n",
    "            \"head_pitch\": 0,\n",
    "            \"head_yaw\": 0,\n",
    "            \"head_roll\": 0,\n",
    "            \"head_pose\": \"None\"\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            \"head_pitch\": pitch,\n",
    "            \"head_yaw\": yaw,\n",
    "            \"head_roll\": roll,\n",
    "            \"head_pose\": head_pose_label(pitch, yaw)\n",
    "        })\n",
    "\n",
    "\n",
    "    # -------- HANDS --------\n",
    "    hands = hand_landmarker.detect(mp_img)\n",
    "    features[\"hand_count\"] = len(hands.hand_landmarks)\n",
    "\n",
    "    features.update({\n",
    "        \"left_hand_x\": 0,\n",
    "        \"left_hand_y\": 0,\n",
    "        \"right_hand_x\": 0,\n",
    "        \"right_hand_y\": 0,\n",
    "        \"hand_obj_interaction\": 0\n",
    "    })\n",
    "\n",
    "    if hands.hand_landmarks:\n",
    "        wrist = hands.hand_landmarks[0][0]\n",
    "        features[\"left_hand_x\"] = wrist.x\n",
    "        features[\"left_hand_y\"] = wrist.y\n",
    "\n",
    "        mouth_pt = (features[\"mouth_x\"], features[\"mouth_y\"])\n",
    "        wrist_px = (wrist.x * w, wrist.y * h)\n",
    "\n",
    "        if euclidean(mouth_pt, wrist_px) < 80:\n",
    "            features[\"hand_obj_interaction\"] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "# =========================\n",
    "# TEST RUN\n",
    "# =========================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_face: 1\n",
      "Count: 1\n",
      "face_present: 1\n",
      "Count: 2\n",
      "face_x: 473\n",
      "Count: 3\n",
      "face_y: 285\n",
      "Count: 4\n",
      "face_w: 305\n",
      "Count: 5\n",
      "face_h: 305\n",
      "Count: 6\n",
      "face_conf: 70.30482888221741\n",
      "Count: 7\n",
      "left_eye_x: 533.1682968139648\n",
      "Count: 8\n",
      "left_eye_y: 370.53189754486084\n",
      "Count: 9\n",
      "right_eye_x: 705.1795196533203\n",
      "Count: 10\n",
      "right_eye_y: 360.4400110244751\n",
      "Count: 11\n",
      "nose_tip_x: 636.0640335083008\n",
      "Count: 12\n",
      "nose_tip_y: 439.9025774002075\n",
      "Count: 13\n",
      "mouth_x: 636.6813659667969\n",
      "Count: 14\n",
      "mouth_y: 497.76546478271484\n",
      "Count: 15\n",
      "pupil_left_x: 558.5663604736328\n",
      "Count: 16\n",
      "pupil_left_y: 369.1128158569336\n",
      "Count: 17\n",
      "pupil_right_x: 681.1087799072266\n",
      "Count: 18\n",
      "pupil_right_y: 362.22928047180176\n",
      "Count: 19\n",
      "gazePoint_x: 619.8375701904297\n",
      "Count: 20\n",
      "gazePoint_y: 365.6710481643677\n",
      "Count: 21\n",
      "gaze_direction: center\n",
      "Count: 22\n",
      "gaze_on_script: 1\n",
      "Count: 23\n",
      "head_pitch: -179.58198741217254\n",
      "Count: 24\n",
      "head_yaw: -14.79146484952375\n",
      "Count: 25\n",
      "head_roll: -3.8087009262764004\n",
      "Count: 26\n",
      "head_pose: left\n",
      "Count: 27\n",
      "hand_count: 0\n",
      "Count: 28\n",
      "left_hand_x: 0\n",
      "Count: 29\n",
      "left_hand_y: 0\n",
      "Count: 30\n",
      "right_hand_x: 0\n",
      "Count: 31\n",
      "right_hand_y: 0\n",
      "Count: 32\n",
      "hand_obj_interaction: 0\n",
      "Count: 33\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# RUN TEST\n",
    "# ==============================\n",
    "count=0\n",
    "if __name__ == \"__main__\":\n",
    "    feats = extract_features(\"input.jpg\")\n",
    "    for k, v in feats.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "        count+=1\n",
    "        print(\"Count:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
