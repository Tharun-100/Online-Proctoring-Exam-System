{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODULE:1 FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: -169.96\n",
      "Yaw  : -10.10\n",
      "Roll : -3.75\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "IMAGE_PATH = \"image.jpg\"          # <-- put your image path\n",
    "MODEL_PATH = r\"E:\\Projects in ML\\FRAUD DETECTION SYSTEM FOR THE ONLINE PROCTORED EXAMS\\src\\models\\face_landmarker.task\"\n",
    "# -----------------------------\n",
    "# Initialize Face Landmarker\n",
    "# -----------------------------\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=vision.RunningMode.IMAGE,\n",
    "    num_faces=1\n",
    ")\n",
    "face_mesh = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "# -----------------------------\n",
    "# Load image\n",
    "# -----------------------------\n",
    "\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "h, w = image.shape[:2]\n",
    "\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "\n",
    "# -----------------------------\n",
    "# Detect landmarks\n",
    "# -----------------------------\n",
    "result = face_mesh.detect(mp_image)\n",
    "\n",
    "if not result.face_landmarks:\n",
    "    print(\"No face detected\")\n",
    "    exit()\n",
    "\n",
    "landmarks = result.face_landmarks[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 2D image points\n",
    "# -----------------------------\n",
    "image_points = np.array([\n",
    "    (landmarks[1].x * w,   landmarks[1].y * h),   # Nose tip\n",
    "    (landmarks[152].x * w, landmarks[152].y * h), # Chin\n",
    "    (landmarks[33].x * w,  landmarks[33].y * h),  # Left eye\n",
    "    (landmarks[263].x * w, landmarks[263].y * h), # Right eye\n",
    "    (landmarks[61].x * w,  landmarks[61].y * h),  # Left mouth\n",
    "    (landmarks[291].x * w, landmarks[291].y * h)  # Right mouth\n",
    "], dtype=np.float64)\n",
    "\n",
    "# -----------------------------\n",
    "# 3D model points\n",
    "# -----------------------------\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -63.6, -12.5),\n",
    "    (-43.3, 32.7, -26.0),\n",
    "    (43.3, 32.7, -26.0),\n",
    "    (-28.9, -28.9, -24.1),\n",
    "    (28.9, -28.9, -24.1)\n",
    "], dtype=np.float64)\n",
    "\n",
    "# -----------------------------\n",
    "# Camera matrix\n",
    "# -----------------------------\n",
    "\n",
    "focal_length = w\n",
    "center = (w / 2, h / 2)\n",
    "\n",
    "camera_matrix = np.array([\n",
    "    [focal_length, 0, center[0]],\n",
    "    [0, focal_length, center[1]],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float64)\n",
    "\n",
    "dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "# -----------------------------\n",
    "# Solve PnP\n",
    "# -----------------------------\n",
    "\n",
    "success, rvec, tvec = cv2.solvePnP(\n",
    "    model_points,\n",
    "    image_points,\n",
    "    camera_matrix,\n",
    "    dist_coeffs,\n",
    "    flags=cv2.SOLVEPNP_ITERATIVE\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to Euler angles\n",
    "# -----------------------------\n",
    "rmat, _ = cv2.Rodrigues(rvec)\n",
    "sy = np.sqrt(rmat[0, 0]**2 + rmat[1, 0]**2)\n",
    "\n",
    "pitch = np.degrees(np.arctan2(rmat[2, 1], rmat[2, 2]))\n",
    "yaw   = np.degrees(np.arctan2(-rmat[2, 0], sy))\n",
    "roll  = np.degrees(np.arctan2(rmat[1, 0], rmat[0, 0]))\n",
    "\n",
    "print(f\"Pitch: {pitch:.2f}\")\n",
    "print(f\"Yaw  : {yaw:.2f}\")\n",
    "print(f\"Roll : {roll:.2f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Draw head pose axis\n",
    "# -----------------------------\n",
    "\n",
    "nose = (int(image_points[0][0]), int(image_points[0][1]))\n",
    "\n",
    "axis_3d = np.float32([\n",
    "    [50, 0, 0],\n",
    "    [0, 50, 0],\n",
    "    [0, 0, 50]\n",
    "])\n",
    "\n",
    "axis_2d, _ = cv2.projectPoints(\n",
    "    axis_3d, rvec, tvec, camera_matrix, dist_coeffs\n",
    ")\n",
    "\n",
    "p_x = tuple(axis_2d[0].ravel().astype(int))\n",
    "p_y = tuple(axis_2d[1].ravel().astype(int))\n",
    "p_z = tuple(axis_2d[2].ravel().astype(int))\n",
    "\n",
    "cv2.line(image, nose, p_x, (0, 0, 255), 2)  # X - red\n",
    "cv2.line(image, nose, p_y, (0, 255, 0), 2)  # Y - green\n",
    "cv2.line(image, nose, p_z, (255, 0, 0), 2)  # Z - blue\n",
    "\n",
    "cv2.putText(\n",
    "    image,\n",
    "    f\"Pitch:{pitch:.1f} Yaw:{yaw:.1f} Roll:{roll:.1f}\",\n",
    "    (10, 30),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    0.7,\n",
    "    (255, 255, 255),\n",
    "    2\n",
    ")\n",
    "\n",
    "cv2.imshow(\"Head Pose Estimation\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze Features need to extracted from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_gaze_features(\n",
    "    self,\n",
    "    face_landmarks,\n",
    "    width: int,\n",
    "    height: int\n",
    ") -> Dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Eye Gaze Tracking using Face Mesh + solvePnP + projectPoints\n",
    "    \"\"\"\n",
    "    result = face_landmarker.detect(mp_image)\n",
    "    face_landmarks = result.face_landmarks[0]\n",
    "    features = {\n",
    "        \"gaze_point_x\": 0.0,\n",
    "        \"gaze_point_y\": 0.0,\n",
    "        \"gaze_direction\": \"None\",\n",
    "        \"gaze_on_script\": 0,\n",
    "        \"left_pupil_x\": 0.0,\n",
    "        \"left_pupil_y\": 0.0,\n",
    "        \"right_pupil_x\": 0.0,\n",
    "        \"right_pupil_y\": 0.0,\n",
    "    }\n",
    "\n",
    "    if face_landmarks is None:\n",
    "        return features\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3D face model points (mm)\n",
    "    # -----------------------------\n",
    "    face_3d = np.array([\n",
    "        (0.0, 0.0, 0.0),        # Nose tip\n",
    "        (0.0, -63.6, -12.5),    # Chin\n",
    "        (-43.3, 32.7, -26.0),   # Left eye\n",
    "        (43.3, 32.7, -26.0),    # Right eye\n",
    "        (-28.9, -28.9, -24.1),  # Left mouth\n",
    "        (28.9, -28.9, -24.1)    # Right mouth\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Corresponding 2D points\n",
    "    # -----------------------------\n",
    "    image_points = np.array([\n",
    "        (face_landmarks[1].x * width, face_landmarks[1].y * height),\n",
    "        (face_landmarks[152].x * width, face_landmarks[152].y * height),\n",
    "        (face_landmarks[33].x * width, face_landmarks[33].y * height),\n",
    "        (face_landmarks[263].x * width, face_landmarks[263].y * height),\n",
    "        (face_landmarks[61].x * width, face_landmarks[61].y * height),\n",
    "        (face_landmarks[291].x * width, face_landmarks[291].y * height),\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Camera matrix\n",
    "    # -----------------------------\n",
    "    focal_length = width\n",
    "    cam_matrix = np.array([\n",
    "        [focal_length, 0, width / 2],\n",
    "        [0, focal_length, height / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Solve head pose\n",
    "    # -----------------------------\n",
    "    success, rvec, tvec = cv2.solvePnP(\n",
    "        face_3d,\n",
    "        image_points,\n",
    "        cam_matrix,\n",
    "        dist_coeffs,\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE\n",
    "    )\n",
    "\n",
    "    if not success:\n",
    "        return features\n",
    "\n",
    "    # -----------------------------\n",
    "    # Pupil coordinates\n",
    "    # -----------------------------\n",
    "    left_pupil = face_landmarks[468]\n",
    "    right_pupil = face_landmarks[473]\n",
    "\n",
    "    lp_x, lp_y = left_pupil.x * width, left_pupil.y * height\n",
    "    rp_x, rp_y = right_pupil.x * width, right_pupil.y * height\n",
    "\n",
    "    features[\"left_pupil_x\"] = lp_x\n",
    "    features[\"left_pupil_y\"] = lp_y\n",
    "    features[\"right_pupil_x\"] = rp_x\n",
    "    features[\"right_pupil_y\"] = rp_y\n",
    "\n",
    "    # -----------------------------\n",
    "    # Gaze ray projection\n",
    "    # -----------------------------\n",
    "    gaze_3d = np.array([[0, 0, 1000.0]], dtype=np.float64)\n",
    "    gaze_2d, _ = cv2.projectPoints(\n",
    "        gaze_3d,\n",
    "        rvec,\n",
    "        tvec,\n",
    "        cam_matrix,\n",
    "        dist_coeffs\n",
    "    )\n",
    "\n",
    "    gaze_x = int(gaze_2d[0][0][0])\n",
    "    gaze_y = int(gaze_2d[0][0][1])\n",
    "\n",
    "    features[\"gaze_point_x\"] = gaze_x\n",
    "    features[\"gaze_point_y\"] = gaze_y\n",
    "\n",
    "    # -----------------------------\n",
    "    # Gaze direction\n",
    "    # -----------------------------\n",
    "    dx = gaze_x - width / 2\n",
    "    dy = gaze_y - height / 2\n",
    "\n",
    "    if abs(dx) < 40 and abs(dy) < 40:\n",
    "        direction = \"center\"\n",
    "    elif abs(dx) > abs(dy):\n",
    "        direction = \"right\" if dx > 0 else \"left\"\n",
    "    else:\n",
    "        direction = \"down\" if dy > 0 else \"up\"\n",
    "\n",
    "    features[\"gaze_direction\"] = direction\n",
    "\n",
    "    # -----------------------------\n",
    "    # Script detection (strong signal)\n",
    "    # -----------------------------\n",
    "    if direction == \"down\" and gaze_y > height * 0.65:\n",
    "        features[\"gaze_on_script\"] = 1\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 478 face landmarks\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "MODEL_PATH = r\"E:\\Projects in ML\\FRAUD DETECTION SYSTEM FOR THE ONLINE PROCTORED EXAMS\\src\\models\\face_landmarker.task\"\n",
    "IMAGE_PATH = \"image.jpg\"\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Face Landmarker\n",
    "# -----------------------------\n",
    "base_options = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
    "\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=False,\n",
    "    output_facial_transformation_matrixes=False,\n",
    "    num_faces=1\n",
    ")\n",
    "\n",
    "face_landmarker = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "# -----------------------------\n",
    "# Read Image\n",
    "# -----------------------------\n",
    "bgr = cv2.imread(IMAGE_PATH)\n",
    "if bgr is None:\n",
    "    raise ValueError(\"Image not found\")\n",
    "\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "h, w, _ = rgb.shape\n",
    "\n",
    "mp_image = mp.Image(\n",
    "    image_format=mp.ImageFormat.SRGB,\n",
    "    data=rgb\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Detect Face Landmarks\n",
    "# -----------------------------\n",
    "result = face_landmarker.detect(mp_image)\n",
    "\n",
    "if not result.face_landmarks:\n",
    "    print(\"No face detected\")\n",
    "    exit()\n",
    "\n",
    "face_landmarks = result.face_landmarks[0]  # 468 landmarks\n",
    "\n",
    "print(f\"Detected {len(face_landmarks)} face landmarks\")\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to pixel coordinates\n",
    "# -----------------------------\n",
    "landmarks_px = []\n",
    "for lm in face_landmarks:\n",
    "    x = int(lm.x * w)\n",
    "    y = int(lm.y * h)\n",
    "    z = lm.z\n",
    "    landmarks_px.append((x, y, z))\n",
    "\n",
    "# -----------------------------\n",
    "# Visualize landmarks\n",
    "# -----------------------------\n",
    "for (x, y, _) in landmarks_px:\n",
    "    cv2.circle(bgr, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "cv2.imshow(\"Face Landmarks\", bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
